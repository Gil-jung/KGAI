# 나이브 베이즈 분류기 (Naive Bayes Classifier)

나이브 베이즈 분류기(Naive Bayes Classifier, NBC)는 확률적 분류 알고리즘으로, 베이즈 정리를 기반으로 하며, 특징들 간의 독립성을 가정하여 간단하면서도 효과적인 분류를 수행합니다. 이 알고리즘은 텍스트 분류, 스팸 필터링, 의료 진단 등 다양한 분야에서 널리 사용됩니다.

## 목차

1. [개요](#개요)
2. [수학적 배경](#수학적-배경)
   - [베이즈 정리](#베이즈-정리)
   - [독립성 가정](#독립성-가정)
3. [모델의 동작 원리](#모델의-동작-원리)
   - [학습 단계](#학습-단계)
   - [예측 단계](#예측-단계)
4. [파라미터 추정](#파라미터-추정)
   - [사전 확률 추정](#사전-확률-추정)
   - [우도 함수 추정](#우도-함수-추정)
5. [분류 결정 규칙](#분류-결정-규칙)
6. [장단점](#장단점)
7. [응용 사례](#응용-사례)
8. [참고문헌](#참고문헌)
9. [요약](#요약)

## 개요

나이브 베이즈 분류기는 각 클래스에 속할 확률을 계산하여 데이터를 분류하는 지도 학습 알고리즘입니다. "나이브(naive)"라는 용어는 모든 특징들이 서로 독립적이라는 단순한 가정을 의미하며, 이 가정을 통해 계산을 간소화합니다. 이 알고리즘은 계산이 빠르고, 적은 데이터로도 좋은 성능을 발휘하며, 특히 고차원 데이터에서 효과적입니다.

## 수학적 배경

### 베이즈 정리

나이브 베이즈 분류기는 베이즈 정리를 기반으로 합니다. 베이즈 정리는 다음과 같이 정의됩니다:

$$
p(y| \mathbf{x}) = \frac{p(\mathbf{x}|y) \, p(y)}{p(\mathbf{x})}
$$

여기서,
- \( p(y| \mathbf{x}) \)는 입력 데이터 \( \mathbf{x} \)가 주어졌을 때 클래스 \( y \)에 속할 사후 확률입니다.
- \( p(\mathbf{x}|y) \)는 클래스 \( y \)가 주어졌을 때 입력 데이터 \( \mathbf{x} \)가 관측될 조건부 확률입니다.
- \( p(y) \)는 클래스 \( y \)의 사전 확률입니다.
- \( p(\mathbf{x}) \)는 입력 데이터 \( \mathbf{x} \)의 전체 확률입니다.

### 독립성 가정

나이브 베이즈 분류기의 핵심 가정은 입력 특징들 간의 독립성입니다. 즉, 각 특징 \( x_i \)가 클래스 \( y \)에 대해 서로 독립적이라고 가정합니다:

$$
p(\mathbf{x}|y) = \prod_{i=1}^{d} p(x_i | y)
$$

여기서 \( d \)는 입력 데이터의 차원(특징의 수)입니다. 이 가정을 통해 조건부 확률 \( p(\mathbf{x}|y) \)를 쉽게 계산할 수 있습니다.

## 모델의 동작 원리

### 학습 단계

1. **데이터 수집**: 라벨이 부착된 학습 데이터를 준비합니다.
2. **사전 확률 계산**: 각 클래스 \( y \)의 사전 확률 \( p(y) \)을 계산합니다.
3. **우도 함수 계산**: 각 클래스 \( y \)에 대해, 각 특징 \( x_i \)의 조건부 확률 \( p(x_i | y) \)을 계산합니다.
4. **파라미터 저장**: 계산된 사전 확률과 조건부 확률을 저장하여 예측 단계에서 사용합니다.

### 예측 단계

1. **사후 확률 계산**: 새로운 데이터 포인트 \( \mathbf{x} \)에 대해 각 클래스 \( y \)의 사후 확률 \( p(y| \mathbf{x}) \)을 계산합니다.
2. **클래스 할당**: 가장 높은 사후 확률을 가진 클래스로 데이터 포인트를 분류합니다.

$$
\hat{y} = \arg\max_{y} \, p(y| \mathbf{x}) = \arg\max_{y} \, p(y) \prod_{i=1}^{d} p(x_i | y)
$$

## 파라미터 추정

### 사전 확률 추정

클래스 \( y \)의 사전 확률 \( p(y) \)은 학습 데이터에서 해당 클래스에 속하는 데이터 포인트의 비율로 추정됩니다.

$$
p(y) = \frac{N_y}{N}
$$

여기서,
- \( N_y \)는 클래스 \( y \)에 속하는 데이터 포인트의 수입니다.
- \( N \)은 전체 데이터 포인트의 수입니다.

### 우도 함수 추정

각 클래스 \( y \)에 대해, 각 특징 \( x_i \)의 조건부 확률 \( p(x_i | y) \)을 추정합니다. 특징의 종류에 따라 다르게 처리되는데, 대표적으로 다음과 같은 방법이 있습니다:

- **이산형 특징**: 각 특징의 조건부 확률을 빈도 기반으로 추정합니다.

  $$
  p(x_i = v | y) = \frac{N_{y,v} + 1}{N_y + V}
  $$

  여기서,
  - \( N_{y,v} \)는 클래스 \( y \)에 속하면서 특징 \( x_i \)가 값 \( v \)인 데이터 포인트의 수입니다.
  - \( V \)는 가능한 특징 값의 수입니다(라플라스 스무딩을 위해 1을 더함).

- **연속형 특징**: 조건부 확률을 정규 분포로 가정하고 평균과 분산을 추정합니다.

  $$
  p(x_i | y) = \frac{1}{\sqrt{2 \pi \sigma_{y,i}^2}} \exp\left( -\frac{(x_i - \mu_{y,i})^2}{2 \sigma_{y,i}^2} \right)
  $$

  여기서,
  - \( \mu_{y,i} \)는 클래스 \( y \)에 속하는 데이터의 특징 \( x_i \)의 평균입니다.
  - \( \sigma_{y,i}^2 \)는 클래스 \( y \)에 속하는 데이터의 특징 \( x_i \)의 분산입니다.

## 분류 결정 규칙

나이브 베이즈 분류기의 분류 결정 규칙은 다음과 같습니다:

$$
\hat{y} = \arg\max_{y} \, p(y) \prod_{i=1}^{d} p(x_i | y)
$$

이 규칙은 새로운 데이터 포인트 \( \mathbf{x} \)에 대해 각 클래스의 사전 확률과 조건부 확률을 곱한 값을 비교하여 가장 큰 값을 가진 클래스로 분류합니다.

## 장단점

### 장점

- **단순성**: 알고리즘이 간단하며 구현이 용이합니다.
- **계산 효율성**: 학습과 예측이 빠르며, 대규모 데이터셋에서도 효율적으로 작동합니다.
- **적은 데이터 요구**: 상대적으로 적은 학습 데이터로도 좋은 성능을 발휘할 수 있습니다.
- **확률적 해석**: 각 클래스에 속할 확률을 제공하여 불확실성을 관리할 수 있습니다.

### 단점

- **독립성 가정의 한계**: 특징들 간의 독립성 가정이 실제 데이터와 다를 경우 성능이 저하될 수 있습니다.
- **데이터의 분포에 민감**: 연속형 특징의 경우 정규 분포 가정이 실제 데이터와 맞지 않으면 성능이 떨어질 수 있습니다.
- **희소 데이터 처리**: 특징의 수가 많고 데이터가 희소할 경우 모델의 성능이 저하될 수 있습니다.

## 응용 사례

나이브 베이즈 분류기는 다양한 분야에서 활용됩니다:

- **스팸 필터링**: 이메일의 단어 빈도를 기반으로 스팸 여부를 판단.
- **문서 분류**: 뉴스 기사, 리뷰 등의 텍스트 데이터를 기반으로 주제별로 분류.
- **의료 진단**: 환자의 증상 데이터를 분석하여 질병을 예측.
- **권장 시스템**: 사용자 행동 데이터를 분석하여 상품 추천.
- **생체 인식**: 얼굴, 음성 등 생체 데이터를 분석하여 개인 식별.

## 참고문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.

## 요약

나이브 베이즈 분류기(Naive Bayes Classifier)는 베이즈 정리와 특징들 간의 독립성 가정을 기반으로 한 확률적 분류 알고리즘입니다. 이 알고리즘은 단순하면서도 효과적으로 데이터를 분류할 수 있으며, 특히 텍스트 분류나 스팸 필터링과 같은 고차원 데이터에서 뛰어난 성능을 발휘합니다. 나이브 베이즈 분류기는 계산이 빠르고, 적은 데이터로도 좋은 성능을 보이지만, 특징들 간의 독립성 가정이 실제 데이터와 맞지 않을 경우 성능이 저하될 수 있는 단점도 가지고 있습니다. 다양한 응용 분야에서 활용 가능하며, 확률적 해석을 통해 불확실성을 관리할 수 있는 장점이 있습니다.