# LDA와 로지스틱 회귀 사이의 연결점

선형 판별 분석(Linear Discriminant Analysis, LDA)과 로지스틱 회귀(Logistic Regression)는 모두 이진 분류 문제에서 널리 사용되는 지도 학습 알고리즘입니다. 이 두 방법은 여러 공통점과 차이점을 가지며, 특정 가정 하에서 유사한 결과를 도출할 수 있습니다. 이 문서에서는 LDA와 로지스틱 회귀 사이의 연결점을 상세히 살펴보고, 두 모델의 수학적 배경, 유사점, 차이점, 그리고 응용 사례를 통해 그 관계를 이해하고자 합니다.

## 목차

1. [개요](#개요)
2. [수학적 배경](#수학적-배경)
   - [선형 판별 분석 (LDA)](#선형-판별-분석-lda)
   - [로지스틱 회귀](#로지스틱-회귀)
3. [유사점](#유사점)
4. [차이점](#차이점)
5. [결정 경계의 형태](#결정-경계의-형태)
6. [파라미터 추정](#파라미터-추정)
7. [모델 선택 시 고려 사항](#모델-선택-시-고려-사항)
8. [응용 사례](#응용-사례)
9. [참고문헌](#참고문헌)

## 개요

LDA와 로지스틱 회귀는 모두 선형 결정 경계를 사용하여 데이터를 분류합니다. 그러나 이 두 방법은 데이터의 분포에 대한 가정과 파라미터 추정 방식에서 차이를 보입니다. 특정 조건 하에서는 두 모델이 동일한 결정 경계를 생성할 수 있지만, 일반적으로는 다른 접근 방식을 취합니다.

## 수학적 배경

### 선형 판별 분석 (LDA)

선형 판별 분석은 각 클래스가 다변량 정규 분포를 따른다는 가정 하에 작동합니다. LDA는 다음과 같은 가정을 기반으로 합니다:

1. **클래스별 정규 분포**: 각 클래스 \( y=k \)는 평균 벡터 \( \mu_k \)와 공분산 행렬 \( \Sigma \)을 가지는 다변량 정규 분포를 따른다.
2. **공통 공분산 행렬**: 모든 클래스가 동일한 공분산 행렬 \( \Sigma \)을 공유한다.

LDA의 목표는 클래스 간의 분산을 최대화하고 클래스 내의 분산을 최소화하는 선형 결정을 찾는 것입니다. 베이즈 정리를 이용하여 포스터리어 확률을 계산하고, 이를 통해 분류 결정을 내립니다.

$$
p(y=k|\mathbf{x}) = \frac{p(\mathbf{x}|y=k) \, p(y=k)}{p(\mathbf{x})}
$$

여기서 \( p(\mathbf{x}|y=k) \)는 다변량 정규 분포로 모델링됩니다.

### 로지스틱 회귀

로지스틱 회귀는 입력 변수와 출력 변수 간의 관계를 시그모이드 함수를 이용하여 모델링합니다. 로지스틱 회귀는 다음과 같은 결정 함수를 사용합니다:

$$
p(y=1|\mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w}^T \mathbf{x} + b)}}
$$

여기서 \( \mathbf{w} \)는 가중치 벡터, \( b \)는 편향(bias) 항입니다. 로지스틱 회귀는 최대 우도 추정(Maximum Likelihood Estimation, MLE)을 사용하여 파라미터 \( \mathbf{w} \)와 \( b \)를 추정합니다.

## 유사점

- **선형 결정 경계**: 두 모델 모두 선형 결정을 사용하여 데이터를 분류합니다.
- **이진 분류**: 주로 이진 분류 문제에 사용됩니다.
- **최적화 목표**: 최적화 문제를 통해 모델 파라미터를 추정합니다.
- **확률적 해석**: 두 모델 모두 각 클래스에 속할 확률을 예측합니다.

## 차이점

- **가정의 차이**:
  - **LDA**: 각 클래스가 동일한 공분산 행렬을 가지는 다변량 정규 분포를 따른다고 가정합니다.
  - **로지스틱 회귀**: 데이터 분포에 대한 특정 가정을 하지 않으며, 단순히 결정 경계를 시그모이드 함수로 모델링합니다.
  
- **파라미터 추정 방법**:
  - **LDA**: 최대 우도 추정을 통해 평균 벡터와 공분산 행렬을 추정합니다.
  - **로지스틱 회귀**: 최대 우도 추정을 통해 가중치 벡터와 편향을 추정합니다.

- **결정 경계의 유연성**:
  - **LDA**: 공분산 행렬의 동일성을 가정함으로써 더 엄격한 선형 경계를 생성합니다.
  - **로지스틱 회귀**: 데이터에 대한 가정이 없어 더 유연하게 선형 경계를 형성할 수 있습니다.

## 결정 경계의 형태

LDA와 로지스틱 회귀는 모두 선형 결정 경계를 가지지만, 그 유도 과정과 결정 경계의 파라미터가 다를 수 있습니다.

### LDA의 결정 경계

LDA는 다음과 같이 결정 경계를 도출합니다:

$$
(\mu_1 - \mu_2)^T \Sigma^{-1} \mathbf{x} - \frac{1}{2} \mu_1^T \Sigma^{-1} \mu_1 + \frac{1}{2} \mu_2^T \Sigma^{-1} \mu_2 = \ln \frac{p(y=2)}{p(y=1)}
$$

여기서 \( \mu_1 \)과 \( \mu_2 \)는 각 클래스의 평균 벡터, \( \Sigma \)는 공통 공분산 행렬입니다.

### 로지스틱 회귀의 결정 경계

로지스틱 회귀는 다음과 같이 결정 경계를 설정합니다:

$$
\mathbf{w}^T \mathbf{x} + b = 0
$$

이 경계는 시그모이드 함수의 출력이 0.5일 때 형성됩니다.

## 파라미터 추정

### LDA의 파라미터 추정

LDA는 각 클래스의 평균 벡터 \( \mu_k \)와 공분산 행렬 \( \Sigma \)을 추정합니다. 파라미터 추정은 다음과 같이 진행됩니다:

$$
\mu_k = \frac{1}{N_k} \sum_{i=1}^{N} \mathbb{I}(y^{(i)} = k) \mathbf{x}^{(i)}
$$

$$
\Sigma = \frac{1}{N} \sum_{k=1}^{K} \sum_{i=1}^{N} \mathbb{I}(y^{(i)} = k) (\mathbf{x}^{(i)} - \mu_k)(\mathbf{x}^{(i)} - \mu_k)^T
$$

### 로지스틱 회귀의 파라미터 추정

로지스틱 회귀는 가중치 벡터 \( \mathbf{w} \)와 편향 \( b \)를 추정합니다. 이는 최대 우도 추정을 통해 비선형 최적화 문제를 해결하여 수행됩니다:

$$
\mathcal{L}(\mathbf{w}, b) = \prod_{i=1}^{N} \sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b)^{y^{(i)}} \left(1 - \sigma(\mathbf{w}^T \mathbf{x}^{(i)} + b)\right)^{1 - y^{(i)}}
$$

이를 로그 변환하여 로그 우도 함수를 최대화합니다.

## 모델 선택 시 고려 사항

- **데이터의 분포**: 데이터가 다변량 정규 분포를 따르고, 클래스 간 공분산이 동일하다면 LDA가 적합할 수 있습니다. 반면, 데이터 분포에 대한 특정 가정이 어려운 경우 로지스틱 회귀가 더 유연할 수 있습니다.
- **차원의 수**: 고차원 데이터에서는 로지스틱 회귀가 계산 효율성 면에서 유리할 수 있습니다.
- **해석 가능성**: 두 모델 모두 해석이 용이하지만, LDA는 데이터의 통계적 특성을 기반으로 하기 때문에 특정 상황에서 더 직관적인 해석이 가능합니다.
- **과적합 방지**: 로지스틱 회귀는 정규화를 통해 과적합을 방지하기 용이합니다.

## 응용 사례

- **의료 진단**: 환자의 증상 데이터를 기반으로 질병 유무를 예측할 때 두 모델 모두 사용될 수 있습니다. 데이터가 정규 분포를 따른다면 LDA가 유리할 수 있으며, 그렇지 않다면 로지스틱 회귀가 더 유연할 수 있습니다.
- **스팸 필터링**: 이메일의 특징을 분석하여 스팸 여부를 분류하는 데 두 모델이 모두 활용될 수 있습니다.
- **금융 사기 탐지**: 거래 패턴을 분석하여 사기 여부를 판단하는 데 사용됩니다.
- **이미지 분류**: 이미지의 특징을 기반으로 객체를 분류할 때 두 모델을 적용할 수 있습니다.

## 참고문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.