# 베이즈 결정 이론의 회귀 문제

## 개요

베이즈 결정 이론(Bayesian Decision Theory)은 주어진 데이터와 사전 지식을 기반으로 최적의 결정을 내리기 위한 프레임워크를 제공합니다. 회귀 문제에서는 연속적인 출력 값을 예측하는 것이 목표입니다. 베이즈 결정 이론을 회귀 문제에 적용하면, 예측을 통해 발생할 수 있는 손실을 최소화하기 위해 확률적 접근 방식을 활용하게 됩니다.

## 베이즈 회귀 문제의 정의

베이즈 회귀에서는 주어진 입력 데이터 $X$와 출력 데이터 $Y$에 대해 목표는 새로운 입력 $x^*$에 대한 출력 값 $y^*$의 조건부 분포 $p(y^* | x^*, X, Y)$을 추정하는 것입니다. 이는 다음과 같은 베이즈 정리를 통해 표현됩니다:

$$
p(y^* | x^*, X, Y) = \int p(y^* | x^*, \theta) p(\theta | X, Y) d\theta
$$

여기서 $\theta$는 모델의 파라미터이며, $p(\theta | X, Y)$는 데이터 $X, Y$를 기반으로 한 $\theta$의 사후 분포입니다.

## 손실 함수와 예측

베이즈 결정 이론에서 최적의 예측은 사후 분포에 따라 기대 손실(expected loss)을 최소화하는 것입니다. 회귀 문제에서 가장 일반적인 손실 함수는 **제곱 손실 함수(squared loss function)**입니다:

$$
L(y^*, \hat{y}) = (y^* - \hat{y})^2
$$

여기서 $y^*$는 실제 값, $\hat{y}$는 예측 값입니다. 제곱 손실 함수의 경우, 사후 평균(posterior mean)이 최적의 예측 값을 제공합니다:

$$
\hat{y} = \mathbb{E}[y^* | x^*, X, Y] = \int y^* p(y^* | x^*, X, Y) dy^*
$$

이는 예측 값 $\hat{y}$가 $p(y^* | x^*, X, Y)$의 기대값으로 설정되어야 함을 의미합니다.

## 예시: 선형 회귀에서의 베이즈 접근

선형 회귀 문제를 예로 들어보겠습니다. 데이터 $D = \{(x_n, y_n)\}_{n=1}^N$가 주어졌을 때, 선형 회귀 모델은 다음과 같이 표현됩니다:

$$
y_n = \theta^T x_n + \epsilon_n
$$

여기서 $\epsilon_n$은 평균이 0이고 분산이 $\sigma^2$인 정규분포를 따르는 노이즈입니다. $\theta$는 모델의 파라미터입니다.

베이즈 접근에서는 $\theta$의 사전 분포 $p(\theta)$를 가정하고, 관측된 데이터를 바탕으로 사후 분포 $p(\theta | D)$를 계산합니다. 새로운 입력 $x^*$에 대해 출력 값의 예측은 사후 분포의 기대값으로 주어집니다:

$$
\hat{y}^* = \mathbb{E}[y^* | x^*, D] = \int (\theta^T x^*) p(\theta | D) d\theta
$$

## 불확실성 평가

베이즈 회귀의 중요한 이점 중 하나는 예측에 대한 불확실성을 자연스럽게 포함할 수 있다는 것입니다. 사후 분포 $p(\theta | D)$는 파라미터의 불확실성을 반영하며, 이를 통해 예측의 신뢰 구간(credibility interval)을 계산할 수 있습니다.

예를 들어, 95% 신뢰 구간은 다음과 같이 계산됩니다:

$$
\hat{y}^* \pm 1.96 \cdot \text{std}(y^* | x^*, D)
$$

여기서 $\text{std}(y^* | x^*, D)$는 예측 분포의 표준 편차입니다.

## 결론

베이즈 결정 이론을 회귀 문제에 적용하면, 데이터와 사전 지식을 결합하여 최적의 예측을 제공하고, 예측에 대한 불확실성까지 포함할 수 있습니다. 이를 통해 더 신뢰성 있는 예측과 의사 결정을 지원할 수 있습니다.

## 참고 문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
