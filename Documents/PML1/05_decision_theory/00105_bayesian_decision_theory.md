# 베이즈 결정 이론 (Bayesian Decision Theory)

## 개요

베이즈 결정 이론은 불확실성이 존재하는 상황에서 최적의 결정을 내리기 위한 수학적 프레임워크를 제공합니다. 이 이론은 베이즈 정리를 바탕으로 하여, 주어진 관측 데이터에 대해 가능한 모든 결정을 평가하고, 그 중에서 기대 손실(Expected Loss)을 최소화하는 결정을 선택하는 방법론입니다.

## 베이즈 정리 (Bayes' Theorem)

베이즈 결정 이론의 핵심은 베이즈 정리입니다. 베이즈 정리는 조건부 확률을 계산하는 방법을 제공하며, 다음과 같이 표현됩니다:

$$
P(\theta | D) = \frac{P(D | \theta) P(\theta)}{P(D)}
$$

여기서:
- \( P(\theta | D) \)는 주어진 데이터 \( D \)에 대한 파라미터 \( \theta \)의 사후 확률(Postier Probability)입니다.
- \( P(D | \theta) \)는 우도 함수(Likelihood Function)로, 주어진 파라미터 \( \theta \)일 때 데이터 \( D \)가 관측될 확률입니다.
- \( P(\theta) \)는 사전 확률(Prior Probability)로, 관측 데이터 이전에 파라미터 \( \theta \)에 대해 우리가 가지고 있는 정보입니다.
- \( P(D) \)는 데이터 \( D \)가 관측될 총 확률입니다.

## 손실 함수 (Loss Function)

베이즈 결정 이론에서 각 결정의 질을 평가하기 위해 손실 함수를 정의합니다. 손실 함수 \( L(\theta, a) \)는 참 파라미터 \( \theta \)에 대해 우리가 내린 결정 \( a \)가 얼마나 잘못되었는지를 수량화합니다. 손실 함수는 다양한 형태를 취할 수 있으며, 이는 문제의 특성에 따라 달라집니다.

## 기대 손실과 최적의 결정

베이즈 결정 이론에서는 가능한 모든 결정을 평가하기 위해 기대 손실(Expected Loss)을 계산합니다. 기대 손실은 다음과 같이 정의됩니다:

$$
R(a) = \int_\theta L(\theta, a) P(\theta | D) d\theta
$$

여기서:
- \( R(a) \)는 결정 \( a \)에 대한 기대 손실입니다.
- \( P(\theta | D) \)는 베이즈 정리를 통해 계산된 사후 확률입니다.

최적의 결정 \( a^* \)는 기대 손실을 최소화하는 결정을 의미하며, 다음과 같이 표현됩니다:

$$
a^* = \arg\min_a R(a)
$$

## 예시: 2-분류 문제

예를 들어, 이진 분류 문제에서는 두 가지 결정 \( a_1 \)과 \( a_2 \)가 있을 수 있습니다. 각각의 결정에 대해 손실을 평가하고, 기대 손실을 최소화하는 결정을 선택합니다.

### 비용 민감 분류

이 경우 비용 민감 분류(Cost-Sensitive Classification)를 사용하여 오류에 대한 비용이 다를 때 최적의 결정을 내릴 수 있습니다. 예를 들어, 클래스 1을 잘못 분류하는 비용이 클래스 2를 잘못 분류하는 비용보다 크다면, 이 정보를 베이즈 결정 이론에 반영하여 결정합니다.

## 결론

베이즈 결정 이론은 불확실성 하에서 합리적인 결정을 내릴 수 있는 강력한 프레임워크를 제공합니다. 손실 함수와 사후 확률을 바탕으로 기대 손실을 최소화하는 결정을 내리는 것이 이 이론의 핵심입니다. 이 접근법은 다양한 실제 문제에 적용될 수 있으며, 특히 비용 민감 분류와 같은 상황에서 유용합니다.
