# 소프트맥스 함수 (Softmax Function)

## 개요
소프트맥스 함수는 다중 클래스 분류 문제에서 자주 사용되는 활성화 함수로, 주어진 입력 벡터를 확률 분포로 변환합니다. 이를 통해 각 클래스에 대한 확률을 계산하고, 이 확률의 합은 항상 1이 됩니다. 소프트맥스 함수는 인공 신경망의 출력 층에서 주로 사용되며, 각 클래스에 속할 확률을 구할 수 있게 해줍니다.

## 수식
소프트맥스 함수는 다음과 같은 수식으로 정의됩니다:

$$
\sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

여기서:
- $ \mathbf{z} $는 입력 벡터 (logits)로, $ z_i $는 그 벡터의 $ i $번째 요소입니다.
- $ K $는 클래스의 총 개수입니다.
- $ \sigma(\mathbf{z})_i $는 $ i $번째 클래스에 속할 확률을 나타냅니다.

## 작동 원리
소프트맥스 함수는 입력 벡터의 각 요소에 대해 지수 함수를 적용한 후, 그 값들을 전체 합으로 나눕니다. 이 과정에서 큰 값일수록 더 큰 확률을 가지게 되고, 모든 클래스의 확률 합이 1이 되도록 보장합니다.

예를 들어, $ \mathbf{z} = [2.0, 1.0, 0.1] $이라는 입력 벡터가 주어졌을 때, 각 요소에 지수 함수를 적용하고 합을 구한 뒤 이를 나누면, 각 클래스에 대한 확률을 구할 수 있습니다.

```python
import numpy as np

def softmax(logits):
    exp_logits = np.exp(logits - np.max(logits))  # 수치 안정성을 위해 최대값을 뺌
    probabilities = exp_logits / np.sum(exp_logits)
    return probabilities

logits = np.array([2.0, 1.0, 0.1])
probabilities = softmax(logits)
print("확률:", probabilities)
```

이 코드의 결과는 다음과 같은 확률 분포를 생성합니다:
$$ 확률 = [0.659, 0.242, 0.099] $$

## 특징 및 장점
- **확률 분포 제공**: 소프트맥스 함수는 각 클래스에 대한 명확한 확률 분포를 제공하여, 신경망의 출력 결과를 해석하기 쉽게 합니다.
- **해석 가능성**: 확률로 결과를 표현하기 때문에 결과를 이해하고 설명하기 용이합니다.
- **다중 클래스 분류 문제에 적합**: 소프트맥스 함수는 여러 클래스 중 하나를 예측해야 하는 상황에서 특히 유용합니다.

## 활용 사례
- **이미지 분류**: 이미지가 여러 클래스 중 어느 하나에 속할 확률을 계산하는 데 사용됩니다. 예를 들어, 이미지가 고양이, 강아지, 자동차 중 무엇인지를 예측할 때 유용합니다.
- **스팸 필터링**: 이메일이 스팸인지 아닌지, 또는 다른 카테고리로 분류할 때 사용됩니다.
- **감정 분석**: 텍스트의 감정이 긍정적인지, 부정적인지, 중립적인지를 분류하는 데 사용됩니다.

소프트맥스 함수는 이러한 다중 클래스 분류 문제에서 신경망의 출력 값을 해석 가능한 확률로 변환하는 중요한 도구로, 특히 확률적 해석이 필요한 분야에서 널리 사용됩니다.