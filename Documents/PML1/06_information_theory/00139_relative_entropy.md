# 상대 엔트로피 (KL 발산)

## 개요

상대 엔트로피(Kullback-Leibler Divergence, KL Divergence)는 두 확률 분포 간의 차이를 측정하는 비대칭적 척도입니다. KL 발산은 한 분포가 다른 분포로부터 얼마나 정보적으로 멀리 떨어져 있는지를 나타내며, 주로 정보 이론과 통계학에서 활용됩니다.

## 1. **KL 발산의 정의**

두 개의 확률 분포 $P$와 $Q$가 주어졌을 때, 분포 $P$가 실제 분포이고 $Q$가 근사 분포라고 할 때, KL 발산은 다음과 같이 정의됩니다:

$$
D_{KL}(P \parallel Q) = \int_{-\infty}^{\infty} p(x) \log \left(\frac{p(x)}{q(x)}\right) dx
$$

이 수식에서 $p(x)$는 참 분포 $P$의 확률 밀도 함수(PDF)이고, $q(x)$는 근사 분포 $Q$의 확률 밀도 함수입니다. 이때 KL 발산은 $P$에 대해 $Q$가 얼마나 잘 근사하는지를 측정합니다.

## 2. **KL 발산의 해석**

KL 발산은 분포 $P$와 $Q$가 얼마나 비슷한지를 나타냅니다:

- **$D_{KL}(P \parallel Q) = 0$**: 두 분포가 동일한 경우 KL 발산은 0이 됩니다.
- **$D_{KL}(P \parallel Q) > 0$**: 두 분포가 다를 경우, KL 발산은 항상 양수입니다. 이는 $Q$가 $P$와 얼마나 다르며, $P$가 $Q$로부터 얼마나 정보적으로 멀리 떨어져 있는지를 나타냅니다.
- **비대칭성**: KL 발산은 비대칭적입니다. 즉, $D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$이며, 이는 두 확률 분포 간의 차이를 측정하는 방향에 따라 값이 달라질 수 있음을 의미합니다.

## 3. **정보 손실의 관점**

KL 발산은 주로 정보 손실의 관점에서 해석됩니다. 예를 들어, 실제로 분포 $P$를 따라야 할 데이터를 분포 $Q$로 모델링할 경우, 이로 인한 평균적인 정보 손실이 KL 발산으로 측정됩니다. 이는 통계 모델링과 머신 러닝에서 중요한 개념으로, 모델의 예측이 실제 데이터 분포와 얼마나 잘 일치하는지를 평가하는 데 사용됩니다.

## 4. **KL 발산의 응용**

KL 발산은 다양한 분야에서 활용됩니다:

- **통계학 및 머신 러닝**: 모델 선택, 정보 이론적 근거 제공, 베이즈 추론 등에서 KL 발산은 중요한 도구입니다.
- **자연어 처리**: 텍스트 데이터의 확률 분포를 비교하여 문서 유사도를 측정하는 데 사용됩니다.
- **신경망 학습**: 신경망의 손실 함수로 사용되어 모델의 예측과 실제 분포 간의 차이를 줄이는 데 기여합니다.

## 5. **KL 발산과 다른 척도의 비교**

KL 발산은 지니 지수(Gini index), 교차 엔트로피(Cross-Entropy) 등과 같은 다른 척도와 함께 사용될 수 있습니다. KL 발산은 비대칭적이며, 이로 인해 두 분포 간의 차이를 측정하는 방향에 따라 결과가 달라질 수 있습니다. 이러한 특성은 분포 비교의 목적에 따라 적절한 척도를 선택해야 함을 의미합니다.

## 참고 문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
