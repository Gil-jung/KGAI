# 이산 확률 변수의 엔트로피

## 개요

엔트로피는 정보 이론에서 불확실성을 정량화하는 데 사용되는 중요한 개념입니다. 이산 확률 변수의 엔트로피는 특정 확률 분포에 따라 그 불확실성을 측정하는 도구로 사용됩니다. 클로드 섀넌(Claude Shannon)이 제안한 엔트로피 개념은 특히 데이터 압축, 정보 이론, 통계적 학습에서 널리 활용됩니다.

## 1. **이산 확률 변수의 정의**

이산 확률 변수(discrete random variable)란 특정한 이산적인 값(유한하거나 무한히 많은 수의 값)을 가질 수 있는 확률 변수를 의미합니다. 예를 들어, 동전을 던졌을 때 나오는 앞면과 뒷면의 결과는 이산 확률 변수로 볼 수 있습니다.

확률 변수 $X$가 가질 수 있는 모든 값들의 집합을 $\mathcal{X}$라고 하고, $X$가 특정 값 $x$를 가질 확률을 $P(X = x)$라고 할 때, 이산 확률 변수의 엔트로피는 다음과 같이 정의됩니다.

$$
H(X) = - \sum_{x \in \mathcal{X}} P(x) \log P(x)
$$

여기서:
- $H(X)$는 확률 변수 $X$의 엔트로피입니다.
- $\mathcal{X}$는 $X$가 취할 수 있는 모든 가능한 값들의 집합입니다.
- $P(x)$는 $X$가 값 $x$를 가질 확률입니다.
- 로그(log)는 보통 밑이 2인 로그를 사용하며, 이 경우 엔트로피의 단위는 비트(bits)입니다.

## 2. **엔트로피의 해석**

엔트로피 $H(X)$는 확률 분포의 불확실성을 측정하는 척도입니다. 다음은 엔트로피의 주요 해석입니다.

- **불확실성의 척도**: 엔트로피는 주어진 확률 분포에서 얻을 수 있는 정보의 평균량을 나타냅니다. 엔트로피가 높을수록 확률 변수의 불확실성이 크다는 것을 의미합니다.
- **동일 분포에서의 최대 엔트로피**: 이산 확률 변수 $X$가 가질 수 있는 모든 값들이 동일한 확률을 가질 때, 엔트로피는 최대가 됩니다. 예를 들어, 동전 던지기에서 앞면과 뒷면이 각각 50%의 확률을 가질 경우, 엔트로피는 1비트가 됩니다.
- **엔트로피가 0일 때**: 확률 변수 $X$가 단일한 값을 가질 때, 즉 불확실성이 전혀 없을 때, 엔트로피는 0이 됩니다.

## 3. **엔트로피 계산 예제**

예를 들어, 주사위의 눈금 1에서 6까지의 값이 모두 동일한 확률로 나오는 공정한 주사위를 생각해 봅시다. 주사위의 각 눈금이 나올 확률은 $P(X = x) = \frac{1}{6}$입니다.

이 경우, 엔트로피 $H(X)$는 다음과 같이 계산됩니다.

$$
H(X) = - \sum_{x=1}^{6} \frac{1}{6} \log_2 \frac{1}{6} = - 6 \times \frac{1}{6} \log_2 \frac{1}{6} = \log_2 6 \approx 2.585 \text{ bits}
$$

이 결과는 주사위를 던질 때, 나올 결과에 대한 불확실성을 2.585 비트로 나타낼 수 있음을 의미합니다.

## 4. **엔트로피의 활용**

이산 확률 변수의 엔트로피는 다양한 분야에서 사용됩니다.

- **정보 이론**: 데이터 압축에서 엔트로피는 최적의 압축률을 결정하는 중요한 지표로 사용됩니다.
- **기계 학습**: 의사결정나무(decision tree) 알고리즘에서 정보 이득(Information Gain)은 분할 전에 비해 분할 후 엔트로피가 얼마나 감소했는지를 나타내는 지표로 사용됩니다.
- **통계 물리학**: 엔트로피는 시스템의 무질서도 또는 불확실성을 측정하는 데 사용되며, 이는 열역학에서 중요한 개념입니다.

## 참고 문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
