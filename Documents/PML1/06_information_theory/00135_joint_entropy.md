# 결합 엔트로피 (Joint Entropy)

## 개요

결합 엔트로피(Joint Entropy)는 두 개 이상의 확률 변수의 결합 확률 분포에 대한 불확실성을 측정하는 개념입니다. 이는 정보 이론에서 중요한 역할을 하며, 특히 다변량 데이터의 불확실성을 평가하는 데 사용됩니다.

## 1. **결합 엔트로피의 정의**

두 확률 변수 $X$와 $Y$의 결합 엔트로피 $H(X, Y)$는 다음과 같이 정의됩니다:

$$
H(X, Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} P(X=x, Y=y) \log P(X=x, Y=y)
$$

여기서:
- $P(X=x, Y=y)$는 $X$와 $Y$의 결합 확률 분포를 나타냅니다.
- $\mathcal{X}$와 $\mathcal{Y}$는 각각 $X$와 $Y$가 취할 수 있는 값들의 집합입니다.

결합 엔트로피는 $X$와 $Y$의 결합 분포에 대한 불확실성을 나타내며, 두 변수가 서로 독립적일 때에는 각 변수의 개별 엔트로피의 합으로 표현될 수 있습니다.

## 2. **결합 엔트로피의 특성**

### 독립성
만약 $X$와 $Y$가 서로 독립적이라면, 결합 확률 분포 $P(X, Y)$는 각각의 주변 확률 분포의 곱으로 표현될 수 있습니다:

$$
P(X, Y) = P(X) \cdot P(Y)
$$

이 경우 결합 엔트로피는 각각의 엔트로피의 합과 같아집니다:

$$
H(X, Y) = H(X) + H(Y)
$$

하지만, $X$와 $Y$가 독립적이지 않다면 결합 엔트로피는 $H(X) + H(Y)$보다 작습니다.

### 상호 정보
결합 엔트로피는 상호 정보(Mutual Information)와도 밀접한 관계가 있습니다. 상호 정보는 $X$와 $Y$ 사이의 종속성을 측정하며, 결합 엔트로피와 개별 엔트로피 사이의 차이로 정의됩니다:

$$
I(X; Y) = H(X) + H(Y) - H(X, Y)
$$

이때 상호 정보 $I(X; Y)$는 $X$와 $Y$ 사이의 정보 교환량을 나타내며, $X$와 $Y$가 독립적일 경우 상호 정보는 0이 됩니다.

## 3. **결합 엔트로피의 응용**

결합 엔트로피는 여러 분야에서 중요한 역할을 합니다. 특히 다음과 같은 응용이 있습니다:

- **정보 이론**: 결합 엔트로피는 다중 변수 데이터의 불확실성을 평가하는 데 사용됩니다.
- **통신 이론**: 전송된 신호와 수신된 신호 간의 정보량을 분석할 때 사용됩니다.
- **머신 러닝**: 결합 확률 분포를 모델링하거나 데이터의 상호 의존성을 분석할 때 사용됩니다.

## 4. **결합 엔트로피 계산의 예**

예를 들어, 두 이산 확률 변수 $X$와 $Y$가 각각 두 개의 값만을 가질 수 있다고 가정합시다. $P(X, Y)$가 다음과 같다면:

$$
P(X, Y) = \begin{pmatrix} 
0.1 & 0.2 \\
0.3 & 0.4 
\end{pmatrix}
$$

결합 엔트로피는 다음과 같이 계산됩니다:

$$
H(X, Y) = - \left( 0.1 \log 0.1 + 0.2 \log 0.2 + 0.3 \log 0.3 + 0.4 \log 0.4 \right)
$$

이를 계산하면 결합 엔트로피 값이 나옵니다. 이 값은 $X$와 $Y$의 결합 분포에 대한 불확실성을 나타냅니다.

## 참고 문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
