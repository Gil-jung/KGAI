# 정칙화 (Regularization)

정칙화(Regularization)는 머신러닝 모델의 학습 과정에서 과적합(Overfitting)을 방지하고 모델의 일반화 성능을 향상시키기 위해 사용되는 기법입니다. 정칙화는 모델의 복잡도를 제어하거나 불필요한 파라미터를 억제함으로써 과적합을 줄이는 데 도움을 줍니다.

## 1. 정칙화의 기본 개념

과적합이란 모델이 학습 데이터에 너무 잘 맞아 새로운 데이터에 대해 일반화 성능이 떨어지는 현상을 말합니다. 정칙화는 손실 함수(Loss Function)에 패널티(Penalty) 항을 추가하여 모델의 복잡도를 줄이고, 학습 데이터 외의 새로운 데이터에 대해서도 좋은 성능을 유지할 수 있도록 합니다.

일반적으로 정칙화는 손실 함수에 추가되는 정칙화 항에 따라 두 가지 주요 방법으로 나뉩니다:
- **L1 정칙화 (Lasso)**: 가중치의 절대값 합에 패널티를 부여하는 방법
- **L2 정칙화 (Ridge)**: 가중치의 제곱 합에 패널티를 부여하는 방법

## 2. 정칙화의 수학적 표현

### L2 정칙화 (Ridge)

L2 정칙화는 모델의 손실 함수에 다음과 같은 패널티 항을 추가합니다:

$$
J(\theta) = L(\theta) + \frac{\lambda}{2} \sum_{i=1}^{n} \theta_i^2
$$

여기서:
- $J(\theta)$는 정칙화된 손실 함수입니다.
- $L(\theta)$는 원래의 손실 함수입니다.
- $\lambda$는 정칙화 강도를 조절하는 하이퍼파라미터입니다.
- $\theta_i$는 모델 파라미터입니다.

이 패널티 항은 큰 가중치를 억제하여 모델이 과도하게 복잡해지는 것을 방지합니다.

### L1 정칙화 (Lasso)

L1 정칙화는 다음과 같이 손실 함수에 패널티 항을 추가합니다:

$$
J(\theta) = L(\theta) + \lambda \sum_{i=1}^{n} |\theta_i|
$$

여기서 $\sum_{i=1}^{n} |\theta_i|$는 가중치의 절대값 합으로, 이 패널티 항은 가중치 값이 0이 되도록 유도하여 희소 모델(Sparse Model)을 만듭니다. L1 정칙화는 일부 가중치를 0으로 만들어 특성 선택(feature selection)을 효과적으로 수행합니다.

## 3. 정칙화의 장점과 단점

### 장점
- **과적합 방지**: 정칙화를 통해 모델의 복잡도를 줄이고 과적합을 방지할 수 있습니다.
- **일반화 성능 향상**: 정칙화를 통해 학습된 모델은 새로운 데이터에 대해 더 잘 일반화할 수 있습니다.
- **희소성**: L1 정칙화는 특성 선택 기능을 제공하여 모델의 해석 가능성을 높입니다.

### 단점
- **모델의 선택**: 정칙화 방법을 선택하고 하이퍼파라미터 $\lambda$를 튜닝하는 과정이 필요합니다.
- **계산 복잡도**: 정칙화 항이 추가됨에 따라 계산 복잡도가 증가할 수 있습니다.

## 4. 정칙화의 응용

정칙화는 다양한 머신러닝 알고리즘에 적용될 수 있습니다. 예를 들어, 선형 회귀, 로지스틱 회귀, 신경망 등 다양한 모델에 정칙화를 추가하여 성능을 향상시킬 수 있습니다.

정칙화는 특히 고차원 데이터에서 유용하며, 변수 선택의 한 방법으로도 사용될 수 있습니다. 이는 데이터에 포함된 불필요한 변수들을 자동으로 제거하거나 가중치를 낮춰 모델의 해석 가능성을 높이는 데 기여합니다.

## 참고 문헌
- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
