# 분산 축소 (Variance Reduction)

**분산 축소**(Variance Reduction)는 최적화 문제나 기계 학습 모델에서 추정치의 불확실성을 줄이는 방법을 의미합니다. 특히 확률적 경사하강법(Stochastic Gradient Descent, SGD) 같은 기법에서 자주 사용되며, 모델의 예측 성능을 안정화하고 일반화 성능을 높이는 데 중요한 역할을 합니다.

## 1. 분산 축소의 필요성

확률적 경사하강법(SGD)은 대규모 데이터셋에서 효율적으로 작동하지만, 매 반복마다 무작위로 선택된 미니배치(mini-batch)에 의존하기 때문에 기울기의 추정치에 높은 분산(variance)을 갖는 경향이 있습니다. 이는 모델 파라미터가 학습 과정에서 진동하거나 최적해에 수렴하지 못하는 문제를 일으킬 수 있습니다.

이러한 문제를 해결하기 위해 분산 축소 기법이 도입되며, 이를 통해 모델의 파라미터 추정의 분산을 줄이고, 보다 안정적이고 빠르게 수렴하도록 돕습니다.

## 2. 대표적인 분산 축소 기법

### 2.1 미니배치 기법 (Mini-Batch Method)
미니배치 기법은 데이터셋을 작은 배치로 나누어 각 배치에 대해 기울기를 계산하는 방법입니다. 이 방법은 순수 SGD와 전체 배치 경사하강법 사이의 절충안으로, 기울기의 분산을 줄이면서도 계산 효율성을 유지합니다.

### 2.2 모멘텀 (Momentum)
모멘텀 기법은 현재 기울기와 이전 기울기의 지수 가중 이동 평균(Exponential Moving Average, EMA)을 계산하여, 기울기의 급격한 변화를 완화시킵니다. 이를 통해 기울기의 진동을 줄이고 더 빠르게 수렴하도록 합니다.

모멘텀 기법은 다음과 같은 수식으로 표현됩니다:

$$
v_t = \beta v_{t-1} + (1 - \beta) \nabla_\theta L(\theta_t)
$$

여기서 $v_t$는 모멘텀, $\beta$는 모멘텀 계수, $\nabla_\theta L(\theta_t)$는 $t$번째 반복에서의 기울기입니다.

### 2.3 적응적 학습률 (Adaptive Learning Rate)
적응적 학습률 기법은 학습률을 반복마다 동적으로 조정하여, 기울기의 크기에 따라 학습률을 증가시키거나 감소시킵니다. 대표적인 방법으로 AdaGrad, RMSProp, Adam 등이 있으며, 이들 모두 학습 과정에서 기울기의 분산을 효과적으로 줄입니다.

### 2.4 제어변수 (Control Variates)
제어변수 기법은 기울기 추정치에서 편향이 없는 보정 용어를 사용하여 분산을 줄이는 방법입니다. 이 방법은 주로 몬테카를로 추정(Monte Carlo estimation)에서 사용되며, 추정의 효율성을 높입니다.

## 3. 분산 축소의 수학적 표현

기본적으로, 최적화 과정에서 기울기의 추정치는 다음과 같은 식으로 표현됩니다:

$$
\mathbb{E}[\nabla_\theta \hat{L}(\theta)] = \nabla_\theta L(\theta)
$$

여기서 $\nabla_\theta \hat{L}(\theta)$는 추정된 기울기, $\nabla_\theta L(\theta)$는 실제 기울기입니다.

분산 축소 기법을 적용하면, 추정된 기울기의 분산이 감소하여 더 정확한 추정치를 얻게 됩니다:

$$
\text{Var}[\nabla_\theta \hat{L}(\theta)] \rightarrow \min
$$

이는 학습의 안정성을 높이고, 최적화 알고리즘이 더 빠르게 수렴할 수 있도록 돕습니다.

## 4. 분산 축소의 효과

분산 축소 기법을 통해 얻을 수 있는 주요 이점은 다음과 같습니다:

- **수렴 속도 향상:** 분산이 줄어들면 기울기의 진동이 줄어들어 수렴 속도가 빨라집니다.
- **모델의 일반화 성능 개선:** 불필요한 진동을 줄임으로써 모델이 더 좋은 일반화 성능을 갖도록 합니다.
- **안정성 증가:** 최적화 과정에서의 불확실성을 줄이고, 보다 신뢰할 수 있는 결과를 얻을 수 있습니다.

## 5. 결론

분산 축소는 기계 학습과 최적화에서 매우 중요한 기법으로, 모델의 학습 과정을 더 안정적이고 효율적으로 만들기 위해 다양한 방법들이 사용됩니다. 이러한 방법들은 SGD와 같은 기본적인 최적화 알고리즘을 보완하여, 더 나은 성능을 달성하는 데 중요한 역할을 합니다.

## 참고 문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
