# 단계 크기 (학습률, Learning Rate)

**단계 크기**(Step Size), 혹은 **학습률**(Learning Rate)은 최적화 알고리즘에서 매우 중요한 하이퍼파라미터 중 하나로, 매 반복(iteration)마다 파라미터를 얼마나 크게 업데이트할지를 결정합니다. 이는 특히 경사하강법(Gradient Descent)과 같은 알고리즘에서 핵심적으로 사용됩니다.

## 1. 단계 크기의 정의

단계 크기 $\alpha$는 다음과 같은 형태로 나타납니다:

$$
\theta_{t+1} = \theta_t - \alpha \nabla f(\theta_t)
$$

여기서:
- $\theta_t$는 현재 시점 $t$에서의 파라미터 값입니다.
- $\alpha$는 단계 크기(학습률)입니다.
- $\nabla f(\theta_t)$는 목적 함수 $f$의 현재 파라미터에서의 기울기(그라디언트)입니다.

## 2. 단계 크기의 역할

단계 크기는 최적화 과정에서 다음과 같은 역할을 합니다:

- **수렴 속도 조절**: 학습률이 너무 크면 최적해를 넘어서 발산할 위험이 있으며, 반대로 너무 작으면 수렴 속도가 매우 느려집니다. 적절한 학습률은 빠르게 수렴하면서도 최적해에 도달하는 데 필수적입니다.
  
- **목적 함수의 변화량 조절**: 학습률은 매번 파라미터가 업데이트되는 정도를 결정하므로, 학습률이 클수록 파라미터가 크게 변하고, 작을수록 천천히 변화합니다.

## 3. 학습률의 선택

학습률을 선택하는 것은 최적화 알고리즘의 성능에 직접적인 영향을 미칩니다. 학습률을 설정할 때 고려해야 할 사항은 다음과 같습니다:

- **고정 학습률**: 학습률을 일정하게 유지하는 방법입니다. 이 경우, 최적의 학습률을 찾기 위해 여러 값을 실험해보아야 합니다.
  
- **적응형 학습률**: 반복 과정에서 학습률을 동적으로 조정하는 방법입니다. 예를 들어, 학습률을 점진적으로 감소시켜 수렴을 촉진할 수 있습니다.

- **모멘텀과 결합**: 모멘텀(momentum)을 사용해 학습률을 보완할 수 있습니다. 이는 현재 기울기뿐만 아니라 이전 기울기를 고려하여 파라미터를 업데이트하므로 더 빠르고 안정적인 수렴을 도울 수 있습니다.

## 4. 학습률의 영향

- **큰 학습률**:
  - 장점: 빠르게 수렴할 가능성이 있음.
  - 단점: 최적해를 지나치거나 발산할 위험이 있음.
  
- **작은 학습률**:
  - 장점: 더 안정적으로 수렴할 가능성이 있음.
  - 단점: 수렴 속도가 느리고, 최적화가 오랜 시간 걸릴 수 있음.

## 5. 학습률 조정 기법

학습률을 효과적으로 조정하기 위한 다양한 기법들이 제안되었습니다:

- **Learning Rate Schedules**: 학습률을 점진적으로 감소시키는 방법. 예를 들어, 매 에포크(epoch)마다 학습률을 일정 비율로 감소시키는 방법이 있습니다.
  
- **Adaptive Learning Rates**: AdaGrad, RMSProp, Adam 등의 방법이 있으며, 이들은 학습률을 데이터에 맞게 적응적으로 조정합니다.

## 6. 실제 적용에서의 고려사항

- **데이터 크기 및 특성**: 큰 데이터셋에서는 작은 학습률을 사용하는 것이 일반적입니다.
- **모델 복잡도**: 복잡한 모델은 더 작은 학습률을 필요로 할 수 있습니다.
- **배치 크기**: 큰 배치(batch)에서는 큰 학습률을 사용할 수 있지만, 작은 배치에서는 작은 학습률이 더 적합할 수 있습니다.

## 참고 문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
