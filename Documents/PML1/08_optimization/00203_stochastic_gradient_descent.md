# 확률적 경사하강법 (Stochastic Gradient Descent, SGD)

**확률적 경사하강법**(Stochastic Gradient Descent, SGD)은 기계 학습과 최적화 문제에서 자주 사용되는 경사하강법의 변형으로, 특히 대규모 데이터셋에서의 효율적인 학습을 가능하게 합니다. 경사하강법은 주어진 목적 함수의 기울기를 따라 최적화 문제를 해결하는 방법으로, SGD는 이 과정을 좀 더 빠르고 효율적으로 수행하도록 설계되었습니다.

## 1. 경사하강법의 개요

경사하강법은 주어진 목적 함수 $f(\theta)$의 최소값을 찾기 위해 기울기(gradient)를 이용해 매 반복(iteration)마다 파라미터 $\theta$를 업데이트하는 방법입니다. 경사하강법의 일반적인 형태는 다음과 같습니다:

$$
\theta_{k+1} = \theta_k - \eta \nabla f(\theta_k)
$$

여기서:
- $\theta_k$는 $k$번째 반복에서의 파라미터 값입니다.
- $\eta$는 학습률(step size) 또는 학습률로, 매 반복마다 파라미터가 이동하는 크기를 조정합니다.
- $\nabla f(\theta_k)$는 목적 함수 $f(\theta)$의 기울기입니다.

경사하강법의 기본 개념은 전체 데이터셋에 대해 기울기를 계산한 후, 그 기울기를 따라 파라미터를 업데이트하는 것입니다. 이 과정은 데이터셋이 커질수록 계산 비용이 많이 들게 됩니다.

## 2. 확률적 경사하강법 (SGD)의 개념

SGD는 경사하강법의 단점을 보완하기 위해 고안된 방법으로, 전체 데이터셋이 아니라 **각 데이터 포인트** 또는 **미니배치**(데이터셋의 작은 부분집합)에 대해 기울기를 계산하고, 이를 기반으로 파라미터를 업데이트합니다.

SGD의 업데이트 식은 다음과 같습니다:

$$
\theta_{k+1} = \theta_k - \eta \nabla f_i(\theta_k)
$$

여기서:
- $f_i(\theta_k)$는 $i$번째 데이터 포인트에 대해 계산된 손실 함수입니다.
- $\nabla f_i(\theta_k)$는 $f_i(\theta_k)$의 기울기입니다.

이 과정은 데이터셋의 크기와 상관없이 기울기를 빠르게 계산할 수 있게 해주며, 따라서 대규모 데이터셋에서도 효율적으로 학습할 수 있습니다.

## 3. SGD의 장점과 단점

### 3.1 장점

- **속도**: 전체 데이터셋에 대해 기울기를 계산하지 않기 때문에 반복당 계산 비용이 낮아지고, 빠르게 학습할 수 있습니다.
- **적응성**: 각 반복에서 무작위로 선택된 데이터 포인트나 미니배치에 대해 기울기를 계산하므로, 지역 최소값에서 벗어나 글로벌 최소값에 도달할 가능성이 높습니다.
- **메모리 효율성**: 한 번에 하나 또는 소수의 데이터 포인트만 메모리에 로드하여 계산하기 때문에 메모리 사용량이 줄어듭니다.

### 3.2 단점

- **기울기의 변동성**: 전체 데이터셋이 아닌 일부 데이터 포인트에 대해 기울기를 계산하기 때문에 기울기의 추정이 불안정할 수 있으며, 이는 최적화 과정의 진동(oscillation)을 초래할 수 있습니다.
- **수렴 속도**: SGD는 반복마다 무작위성을 도입하기 때문에 수렴 속도가 느릴 수 있으며, 특히 학습률을 잘못 설정하면 최적의 해에 도달하지 못할 수도 있습니다.

## 4. SGD의 변형 및 개선 방법

SGD의 단점을 보완하기 위해 여러 가지 변형 기법이 개발되었습니다:

- **모멘텀**: 기울기의 변동성을 줄이고, 보다 빠르게 수렴하기 위해 기울기의 이동 평균을 사용하는 방법입니다.
- **AdaGrad**: 학습률을 각 파라미터에 대해 적응적으로 조정하여, 희소한(sparse) 데이터셋에서 효과적으로 작동할 수 있도록 합니다.
- **RMSprop**: 학습률을 조정하여 수렴 속도를 개선하고, 학습이 안정적으로 진행되도록 하는 방법입니다.
- **Adam**: 모멘텀과 RMSprop의 장점을 결합한 방법으로, 최근 많이 사용되는 최적화 알고리즘 중 하나입니다.

## 5. 결론

확률적 경사하강법(SGD)은 대규모 데이터셋을 다룰 때 효율적이고, 지역 최소값에서 벗어나 글로벌 최소값에 도달할 가능성이 높은 최적화 방법입니다. 비록 기울기의 변동성으로 인해 수렴 속도가 느리거나 불안정할 수 있지만, 모멘텀, AdaGrad, RMSprop, Adam과 같은 개선된 방법을 사용함으로써 이러한 단점을 보완할 수 있습니다.

## 참고 문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
