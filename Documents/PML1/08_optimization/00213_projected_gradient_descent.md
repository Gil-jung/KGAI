# 사영된 경사하강법 (Projected Gradient Descent)

**사영된 경사하강법(Projected Gradient Descent, PGD)**은 제약 조건이 있는 최적화 문제를 해결하기 위한 방법으로, 최적화 과정에서 해가 항상 제약 조건을 만족하도록 합니다. 이는 일반적인 경사하강법이 제약 조건을 고려하지 않는 것과 대조됩니다.

## 1. 문제 설정

사영된 경사하강법은 다음과 같은 형태의 제약 조건이 있는 최적화 문제를 해결하는 데 사용됩니다:

$$
\min_{x \in C} \; f(x)
$$

여기서,
- $f(x)$는 최소화하려는 목적 함수입니다.
- $C$는 제약 조건이 정의된 공간으로, $C \subseteq \mathbb{R}^n$입니다.

이 문제에서 우리는 $f(x)$를 최소화하면서 $x$가 항상 집합 $C$에 속하도록 해야 합니다.

## 2. 사영 연산자

사영된 경사하강법의 핵심은 **사영 연산자(Projection Operator)**를 사용하는 것입니다. 사영 연산자는 주어진 점을 제약 집합 $C$로 사영(Projection)하여 제약을 만족하는 가장 가까운 점을 찾습니다. 사영 연산자는 다음과 같이 정의됩니다:

$$
\text{proj}_{C}(v) = \arg\min_{x \in C} \| x - v \|^2
$$

여기서,
- $\text{proj}_{C}(v)$는 벡터 $v$를 집합 $C$로 사영한 결과입니다.
- $\| \cdot \|$는 유클리드 거리(norm)를 나타냅니다.

사영 연산자는 벡터 $v$가 제약 조건을 벗어났을 때, 이를 다시 제약 조건 내로 끌어오는 역할을 합니다.

## 3. 사영된 경사하강법의 알고리즘

사영된 경사하강법은 다음과 같은 단계로 이루어집니다:

1. 초기화: $x^{(0)}$를 집합 $C$ 내에서 임의로 선택한다.
2. 반복:
   - **경사 계산**: 목적 함수 $f(x)$에 대한 그래디언트 $\nabla f(x^{(k)})$를 계산한다.
   - **경사하강**: 경사하강법을 적용하여 임시 해 $y^{(k+1)}$를 계산한다.
   
     $$
     y^{(k+1)} = x^{(k)} - \lambda^{(k)} \nabla f(x^{(k)})
     $$
   
   - **사영**: 임시 해 $y^{(k+1)}$를 제약 집합 $C$로 사영하여 새로운 해 $x^{(k+1)}$를 구한다.
   
     $$
     x^{(k+1)} = \text{proj}_{C}(y^{(k+1)})
     $$

3. 수렴 조건을 만족할 때까지 반복한다.

여기서 $\lambda^{(k)}$는 학습률 또는 단계 크기입니다.

## 4. 응용 및 장점

사영된 경사하강법은 다음과 같은 상황에서 유용합니다:

- **제약 조건이 있는 최적화 문제**: 예를 들어, 변수의 값이 특정 범위 안에 있어야 하는 문제에서 사용됩니다.
- **볼록 최적화**: 제약 집합 $C$가 볼록(convex) 집합인 경우, 이 방법은 수렴성을 보장합니다.
- **고차원 문제**: 대규모 데이터셋을 처리할 때도 효율적으로 적용할 수 있습니다.

이 방법은 해가 항상 제약 조건을 만족하도록 보장하므로, 제약 조건을 강하게 적용해야 하는 문제에서 특히 유용합니다.

## 5. 예시: 비선형 제약 조건

비선형 제약 조건이 있는 경우에도 사영된 경사하강법을 적용할 수 있습니다. 예를 들어, 제약 조건이 구 또는 단체(sphere or simplex)로 정의된 경우, 해당 제약 집합으로 사영하는 연산은 명시적으로 정의될 수 있습니다.

- **단체(Simplex)**에 대한 사영:
  
  $$
  C = \{x \in \mathbb{R}^n : x_i \geq 0, \sum_{i=1}^n x_i = 1\}
  $$
  
  이 경우, 사영 연산은 각 요소를 비음수로 만들고, 합이 1이 되도록 정규화합니다.

## 6. 결론

사영된 경사하강법은 제약 조건이 있는 최적화 문제를 해결하는 데 있어 강력한 도구입니다. 이 방법은 제약을 만족하는 해를 보장하며, 다양한 제약 조건에 대해 유연하게 적용할 수 있습니다. 경사하강법의 기본 원리에 사영 연산을 결합함으로써, 제약 조건을 명시적으로 처리할 수 있는 강력한 최적화 기법이 됩니다.

## 참고 문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
