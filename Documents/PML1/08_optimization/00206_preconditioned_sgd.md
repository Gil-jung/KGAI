# 선조건부 SGD (Preconditioned SGD)

**선조건부 SGD**(Preconditioned Stochastic Gradient Descent, Preconditioned SGD)는 확률적 경사하강법(SGD)의 변형으로, 경량화된 업데이트를 통해 더 빠르고 안정적인 학습을 목표로 합니다. 이 방법은 각 반복에서 경량화된 업데이트를 적용하기 전에 조건부 선조건부를 사용하는 것으로 특징지어집니다. 선조건부 SGD는 특히 데이터가 크고, 모델이 복잡할 때 유용합니다.

## 1. 선조건부 SGD의 개요

전통적인 확률적 경사하강법(SGD)은 매 반복마다 전체 데이터셋에서 무작위 샘플을 선택하여 모델 파라미터를 업데이트합니다. 하지만 이러한 방법은 데이터셋의 크기와 모델의 복잡성에 따라 계산 비용이 높아질 수 있습니다. 선조건부 SGD는 이러한 문제를 해결하기 위해 업데이트를 수행하기 전에 조건부를 설정하고, 조건이 만족될 때만 파라미터를 업데이트합니다.

## 2. 선조건부 SGD의 기본 원리

선조건부 SGD는 다음과 같은 기본 원리로 작동합니다:

1. **선조건부 설정**: 경량화된 업데이트를 수행하기 전, 특정 조건을 정의합니다. 이 조건은 데이터의 분산, 기울기의 크기, 또는 모델의 변화율에 기반할 수 있습니다.

2. **조건 검토**: 각 반복에서 이 조건을 평가하여 업데이트의 필요성을 결정합니다.

3. **업데이트 수행**: 조건이 만족될 때만 파라미터를 업데이트합니다.

## 3. 수학적 표현

선조건부 SGD는 다음과 같은 수식으로 표현됩니다:

1. **조건부 업데이트**: 모델 파라미터 $\theta$를 업데이트하는 조건을 정의합니다. 이 조건은 다음과 같이 기울기 벡터의 크기 $\| \nabla_\theta L(\theta) \|$ 또는 기울기의 변화율에 따라 결정됩니다.

2. **업데이트 수식**:

$$
\theta_{t+1} = \theta_t - \eta_t H_t^{-1} \nabla_\theta L(\theta_t)
$$

여기서, 
- $\eta_t$는 현재 반복의 학습률,
- $H_t$는 현재 반복에서의 헤세 행렬 또는 선조건부 행렬입니다,
- $\nabla_\theta L(\theta_t)$는 현재 반복에서의 기울기입니다.

## 4. 선조건부 설정의 예

### 4.1 기울기 크기 기반
기울기의 크기가 특정 임계값 $\epsilon$을 초과할 때만 업데이트를 수행합니다:

$$
\| \nabla_\theta L(\theta_t) \| > \epsilon
$$

### 4.2 헤세 행렬 기반
헤세 행렬 $H_t$를 사용하여 업데이트를 조정합니다. 선조건부 행렬 $M_t$는 보통 헤세 행렬의 근사 또는 역행렬로 설정됩니다:

$$
H_t = \nabla^2_\theta L(\theta_t) \text{ 또는 } M_t \approx H_t^{-1}
$$

## 5. 선조건부 SGD의 장점

선조건부 SGD는 다음과 같은 장점을 제공합니다:

- **계산 효율성 향상**: 선조건부를 통해 조건이 충족될 때만 업데이트를 수행하므로 계산 비용을 줄일 수 있습니다.
- **학습 안정성 향상**: 불필요한 업데이트를 줄여 학습 과정을 더 안정적으로 만들 수 있습니다.
- **빠른 수렴**: 조건부 선조건부를 통해 더 빠르고 안정적인 수렴을 이룰 수 있습니다.

## 6. 결론

선조건부 SGD는 전통적인 SGD를 개선한 기법으로, 특정 조건이 충족될 때만 경량화된 업데이트를 수행하여 학습의 효율성과 안정성을 높이는 방법입니다. 이 기법은 대규모 데이터셋과 복잡한 모델에서 특히 유용하며, 학습 속도와 모델의 성능을 향상시키는 데 도움이 됩니다.

## 참고 문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
