# BFGS 및 그 밖의 준뉴턴법

**준뉴턴법**(Quasi-Newton Methods)은 뉴턴법에서 요구하는 헤세 행렬(Hessian matrix)의 직접 계산을 피하기 위해, 헤세 행렬의 근사치를 사용하여 최적화 문제를 해결하는 방법들입니다. 이러한 방법들은 계산 효율성을 높이면서도 높은 수렴 속도를 제공하여, 특히 대규모 또는 복잡한 최적화 문제에서 유용합니다.

## 1. BFGS 알고리즘

BFGS(Broyden-Fletcher-Goldfarb-Shanno) 알고리즘은 준뉴턴법 중 가장 널리 사용되는 알고리즘입니다. BFGS는 현재 점에서의 함수의 기울기 정보를 사용하여, 이전 단계에서 추정한 헤세 행렬의 역행렬을 점진적으로 업데이트함으로써 최적화 문제를 해결합니다.

### 1.1 BFGS 알고리즘의 업데이트 규칙

BFGS 알고리즘에서 헤세 행렬의 역행렬의 근사는 다음과 같은 업데이트 규칙을 통해 이루어집니다:

$$
B_{k+1} = B_k + \frac{y_k y_k^\top}{y_k^\top s_k} - \frac{B_k s_k s_k^\top B_k^\top}{s_k^\top B_k s_k}
$$

여기서:
- $B_k$는 $k$번째 반복에서의 헤세 행렬의 역행렬 근사치입니다.
- $s_k = \theta_{k+1} - \theta_k$는 $k$번째 반복에서의 파라미터 변화량입니다.
- $y_k = \nabla f(\theta_{k+1}) - \nabla f(\theta_k)$는 $k$번째 반복에서의 기울기 변화량입니다.

이 업데이트 규칙은 새로운 정보를 반영하여 헤세 행렬의 근사치를 점진적으로 개선합니다. BFGS 알고리즘은 이러한 방법을 통해 헤세 행렬의 직접 계산 없이도 최적화 문제를 해결할 수 있습니다.

### 1.2 BFGS의 장점

- **계산 효율성**: 헤세 행렬의 역행렬을 직접 계산하지 않기 때문에, 계산 복잡도가 뉴턴법보다 낮습니다.
- **안정성**: 헤세 행렬의 역행렬이 항상 양의 정부호(positive definite)로 유지되도록 보장하므로, 최적화 과정에서 안정성이 높습니다.
- **빠른 수렴**: 초기 값이 적절한 경우, BFGS는 빠른 수렴 속도를 보이며, 대부분의 비선형 최적화 문제에서 매우 효과적입니다.

## 2. L-BFGS (Limited-memory BFGS)

**L-BFGS**(Limited-memory BFGS)는 BFGS 알고리즘의 변형으로, 대규모 최적화 문제를 해결하기 위해 메모리 사용량을 줄이는 방법입니다. L-BFGS는 과거 몇 번의 반복에서 계산된 정보만을 저장하고 사용하여 헤세 행렬의 근사치를 계산합니다.

### 2.1 L-BFGS의 특징

- **메모리 절약**: 헤세 행렬의 전체를 저장하지 않고, 소수의 최근 $m$번의 정보만을 사용하여 메모리 사용량을 대폭 줄입니다.
- **대규모 문제에 적합**: 특히 고차원 데이터나 대규모 최적화 문제에서 효과적입니다.

## 3. 그 밖의 준뉴턴법

### 3.1 DFP 알고리즘

DFP(Davidon-Fletcher-Powell) 알고리즘은 BFGS와 유사한 방식으로 헤세 행렬의 근사치를 업데이트하는 또 다른 준뉴턴법입니다. DFP와 BFGS는 헤세 행렬을 업데이트하는 방식에서 차이를 보이지만, 둘 다 비슷한 수렴 속도를 가지고 있습니다. DFP의 업데이트 규칙은 다음과 같습니다:

$$
B_{k+1} = B_k + \frac{s_k s_k^\top}{s_k^\top y_k} - \frac{B_k y_k y_k^\top B_k^\top}{y_k^\top B_k y_k}
$$

### 3.2 SR1 알고리즘

SR1(Symmetric Rank 1) 알고리즘은 또 다른 준뉴턴법으로, 헤세 행렬의 근사치를 1차 랭크 수정(rank-1 update)을 통해 업데이트합니다. SR1은 BFGS나 DFP와 비교해 더 간단한 구조를 가지지만, 수렴 속도와 안정성에서 차이가 있을 수 있습니다.

## 4. 준뉴턴법의 장점과 단점

### 4.1 장점

- **효율성**: 헤세 행렬의 직접 계산 없이도 빠른 수렴을 제공하므로, 큰 문제에서도 효율적으로 적용될 수 있습니다.
- **유연성**: 다양한 형태의 최적화 문제에 적용 가능하며, 다양한 변형 알고리즘이 존재하여 문제에 맞는 선택이 가능합니다.

### 4.2 단점

- **복잡성**: 알고리즘 구현이 비교적 복잡하며, 각 알고리즘의 파라미터 조정이 필요할 수 있습니다.
- **국소 최적해**: 초기 값에 따라 국소 최적해에 수렴할 수 있으며, 전역 최적화를 보장하지 않습니다.

## 5. 결론

BFGS 및 기타 준뉴턴법은 뉴턴법의 계산 복잡도를 줄이면서도 효율적인 최적화를 제공하는 중요한 방법입니다. 특히, 대규모 비선형 최적화 문제에서 이들 방법은 헤세 행렬의 근사를 통해 빠르고 안정적인 수렴을 보장합니다. BFGS는 그 중에서도 가장 널리 사용되는 방법으로, 다양한 실전 최적화 문제에서 효과적으로 적용될 수 있습니다.

## 참고 문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
