# EM 알고리즘 (Expectation-Maximization Algorithm)

**EM 알고리즘(Expectation-Maximization Algorithm)**은 관찰되지 않은 숨겨진 변수(Latent Variable)가 있는 확률 모델의 파라미터 추정을 위한 반복적인 방법입니다. 주로 최대우도추정(Maximum Likelihood Estimation, MLE) 또는 최대 사후 확률 추정(Maximum A Posteriori, MAP)을 계산하는 데 사용됩니다.

## 1. EM 알고리즘의 필요성

EM 알고리즘은 직접적으로 최적화하기 어려운 잠재 변수를 포함한 확률 모델에서, 이들 잠재 변수를 고려한 모델 파라미터를 효율적으로 추정할 수 있도록 고안되었습니다. 특히, 다음과 같은 상황에서 유용합니다:

- **불완전한 데이터**: 데이터셋의 일부가 누락되거나 관찰되지 않은 경우.
- **복잡한 모델**: 혼합 모델(Mixture Model)과 같이 숨겨진 변수를 포함하는 복잡한 모델에서, 직접적인 최대우도추정이 어려운 경우.

## 2. EM 알고리즘의 기본 개념

EM 알고리즘은 두 가지 주요 단계로 이루어져 있습니다:

### 2.1 E-Step (Expectation Step)
E-Step에서는 주어진 현재 파라미터 추정치에 따라 잠재 변수의 기대값을 계산합니다. 이는 현재 파라미터 추정치를 바탕으로 데이터의 완전 데이터 로그 우도(Complete Data Log-Likelihood)의 기대값을 계산하는 과정입니다.

$$
Q(\theta \mid \theta^{(t)}) = \mathbb{E}_{Z \mid X, \theta^{(t)}}[\log p(X, Z \mid \theta)]
$$

여기서 $X$는 관찰된 데이터, $Z$는 숨겨진 변수, $\theta$는 모델 파라미터, $\theta^{(t)}$는 현재 파라미터 추정치를 의미합니다.

### 2.2 M-Step (Maximization Step)
M-Step에서는 E-Step에서 계산된 기대값을 최대화하는 파라미터 $\theta$를 찾습니다. 이는 다음 단계에서 사용할 새로운 파라미터 추정치를 업데이트하는 과정입니다.

$$
\theta^{(t+1)} = \arg\max_{\theta} Q(\theta \mid \theta^{(t)})
$$

이 과정을 반복하면 파라미터 추정치는 점차 수렴하며, 최종적으로 우도를 최대화하는 파라미터에 도달하게 됩니다.

## 3. EM 알고리즘의 수렴 및 수렴 속도

EM 알고리즘은 각 반복 단계에서 우도가 증가하는 특성을 가지며, 수렴할 때까지 반복합니다. 수렴 속도는 초기 파라미터 설정, 데이터의 특성, 모델의 복잡성에 따라 달라질 수 있습니다.

### 3.1 수렴 기준
일반적으로 EM 알고리즘의 수렴은 두 가지 방법 중 하나로 결정됩니다:

- **우도의 변화가 매우 작아질 때**: 연속된 반복 간 우도 값의 변화가 미미할 때 알고리즘을 종료합니다.
- **파라미터의 변화가 매우 작아질 때**: 연속된 반복 간 파라미터 값의 변화가 기준 이하일 때 알고리즘을 종료합니다.

### 3.2 초기화의 중요성
EM 알고리즘은 비볼록 함수에서 지역 최적점에 빠질 수 있으므로, 초기 파라미터 설정이 매우 중요합니다. 초기화가 적절하지 않을 경우 최적의 파라미터를 찾지 못할 수 있습니다.

## 4. EM 알고리즘의 적용 예시

EM 알고리즘은 다양한 모델과 문제에 적용될 수 있습니다. 대표적인 적용 사례는 다음과 같습니다:

### 4.1 가우시안 혼합 모델 (Gaussian Mixture Model, GMM)
가우시안 혼합 모델은 데이터가 여러 개의 가우시안 분포로부터 생성된 것으로 가정합니다. 여기서 각 데이터가 어느 가우시안 분포로부터 생성되었는지는 관찰되지 않은 숨겨진 변수로 취급되며, EM 알고리즘을 통해 각 분포의 파라미터를 추정합니다.

### 4.2 결측 데이터 처리
데이터셋의 일부가 결측된 경우, EM 알고리즘을 통해 결측값을 추정하여 모델 파라미터를 학습할 수 있습니다.

## 5. 수학적 예시: 가우시안 혼합 모델

가우시안 혼합 모델에서, EM 알고리즘의 각 단계를 구체적으로 설명할 수 있습니다.

### 5.1 E-Step
E-Step에서는 각 데이터 포인트가 특정 가우시안 분포에 속할 확률, 즉 책임도(Responsibility)를 계산합니다.

$$
\gamma_{zn} = \frac{\pi_z \mathcal{N}(x_n \mid \mu_z, \Sigma_z)}{\sum_{k=1}^{K} \pi_k \mathcal{N}(x_n \mid \mu_k, \Sigma_k)}
$$

여기서 $\pi_z$는 혼합 계수, $\mathcal{N}(x_n \mid \mu_z, \Sigma_z)$는 가우시안 분포입니다.

### 5.2 M-Step
M-Step에서는 E-Step에서 계산된 책임도를 이용해 새로운 파라미터를 계산합니다.

$$
\mu_z^{(t+1)} = \frac{\sum_{n=1}^{N} \gamma_{zn} x_n}{\sum_{n=1}^{N} \gamma_{zn}}
$$

$$
\Sigma_z^{(t+1)} = \frac{\sum_{n=1}^{N} \gamma_{zn} (x_n - \mu_z^{(t+1)})(x_n - \mu_z^{(t+1)})^T}{\sum_{n=1}^{N} \gamma_{zn}}
$$

## 6. EM 알고리즘의 장점과 한계

### 6.1 장점
- **일반성**: 다양한 숨겨진 변수 모델에 적용 가능.
- **우도 보장**: 각 반복 단계에서 우도가 증가하는 특성을 가짐.

### 6.2 한계
- **수렴 속도**: 수렴이 느릴 수 있으며, 초기화에 따라 지역 최적점에 빠질 수 있음.
- **복잡한 모델에 대한 계산 비용**: 고차원 문제나 복잡한 모델에서는 계산 비용이 클 수 있음.

## 7. 결론

EM 알고리즘은 숨겨진 변수가 있는 모델에서 최대우도추정 또는 최대 사후 확률 추정을 효율적으로 수행할 수 있는 강력한 도구입니다. 다양한 데이터 분석 및 기계 학습 문제에서 널리 사용되며, 특히 가우시안 혼합 모델과 결측 데이터 처리에 유용합니다. 그러나 수렴 속도 및 초기화에 따른 한계점을 인식하고, 적절한 상황에서 사용해야 합니다.

## 참고 문헌

- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.
- Murphy, K. P. (2022). *Probabilistic Machine Learning: An Introduction*. MIT Press.
- Murphy, K. P. (2023). *Probabilistic Machine Learning: Advanced Topics*. MIT Press.
